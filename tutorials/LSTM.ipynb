{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab242ba",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on notable speeches of the last decade\n",
    "\n",
    "---\n",
    "\n",
    "This tutorial demonstrates how to build a simple <a href = 'https://en.wikipedia.org/wiki/Long_short-term_memory'> Long Short Term memory network (LSTM) </a> from scratch in NumPy to perform sentiment analysis on a socially relevant and ethically acquired dataset.\n",
    "\n",
    "Your deep learning model - The LSTM is a form of a Recurrent Neural Network and will learn to classify a piece of text as positive or negative from the IMDB reviews dataset. The dataset contains 40,000 training and 10,000 test reviews and corresponding labels. Based on the numeric representations of these reviews and their corresponding labels <a href = 'https://en.wikipedia.org/wiki/Supervised_learning'> (supervised learning) </a> the neural network will be trained to learn the sentiment using forward propagation and backpropagaton through time since we are dealing with sequential data here. The output will be a vector containing the probabilities that the text samples are positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f957fab2",
   "metadata": {},
   "source": [
    "Today, Deep Learning is getting adopted in everyday life and now it is more important to ensure that decisions that have been taken using AI are not reflecting discriminatory behavior towards a set of populations. It is important to take fairness into consideration while consuming the output from AI. Throughout the tutorial we'll try to question all the steps in our pipeline from an ethics point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec02e24",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "\n",
    "---\n",
    "\n",
    "You are expected to be familiar with the Python programming language and array manipulation with NumPy. In addition, some understanding of Linear Algebra and Calculus is recommended. You should also be familiar with how Neural Networks work. For reference, you can visit the [Python](https://docs.python.org/dev/tutorial/index.html), [Linear algebra on n-dimensional arrays](https://numpy.org/doc/stable/user/tutorial-svd.html) and [Calculus](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html) tutorials.\n",
    "\n",
    "To get a refresher on Deep Learning basics, You should consider reading [the d2l.ai book](https://d2l.ai/chapter_recurrent-neural-networks/index.html), which is an interactive deep learning book with multi-framework code, math, and discussions. You can also go through the [Deep learning on MNIST from scratch tutorial](https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html) to understand how a basic neural network is implemented from scratch.\n",
    "\n",
    "In addition to NumPy, you will be utilizing the following Python standard modules for data loading and processing:\n",
    "- [`pandas`](https://pandas.pydata.org/docs/) for handling dataframes \n",
    "- [`re`](https://docs.python.org/3/library/re.html) for tokenising textual data \n",
    "- [`string`](https://docs.python.org/3/library/string.html) for string operations  \n",
    "\n",
    "    as well as:\n",
    "- [Matplotlib](https://matplotlib.org/) for data visualization\n",
    "\n",
    "This tutorial can be run locally in an isolated environment, such as [Virtualenv](https://virtualenv.pypa.io/en/stable/) or [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html). You can use [Jupyter Notebook or JupyterLab](https://jupyter.org/install) to run each notebook cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa57362",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "---\n",
    "\n",
    "1. Data Collection \n",
    "\n",
    "2. Preprocess the datasets\n",
    "\n",
    "3. Build and train a LSTM network from scratch\n",
    "\n",
    "4. Perform sentiment analysis on collected speeches \n",
    "\n",
    "5. Next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74aee921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary packages \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from text_preprocessing import TextPreprocess \n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e5c803",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "----\n",
    "\n",
    "Before we begin there are a few pointers you should always keep in mind before choosing the data you wish to train your model on:\n",
    "- **Identifying Data Bias** - Bias is a component of the human thought process, and data collected from humans therefore inherently reflects that bias. Some ways in which this bias tends to occur in Machine Learning datasets are:\n",
    "    - *Bias in historical data*: Historical data are often skewed towards, or against, particular groups.\n",
    "        Data can also be severely imbalanced with limited information on protected groups.\n",
    "    - *Bias in data collection mechanisms*: Lack of representativeness introduces inherent biases in the data collection process.  \n",
    "    - *Bias towards observable outcomes*: In some scenarios, we have the information about True Outcomes only for a certain section of the population. In the absence of information on all outcomes, one cannot even measure fairness\n",
    "- **Preserving human anonymity for sensitive data**: [Trevisan and Reilly](https://eprints.whiterose.ac.uk/91157/1/Ethical%20dilemmas.pdf) identified a list of sensitive topics that need to be handled with extra care. We present the same below along with a few additions:\n",
    "    - personal daily routines (including location data);\n",
    "    - individual details about impairment and/or medical records;\n",
    "    - emotional accounts of pain and chronic illness;\n",
    "    - financial information about income and/or welfare payments;\n",
    "    - discrimination and abuse episodes;\n",
    "    - criticism/praise of individual providers of healthcare and support services\n",
    "    - suicidal thoughts;\n",
    "    - criticism/praise of a power structure especially if it compromises their safety\n",
    "    - personally-identifying information (even if anonymized in some way) including things like fingerprints or voice\n",
    "\n",
    "In this section, you will be collecting two different datasets, the IMDB movie reviews dataset and a collection of 10 speeches curated for this tutorial including activists from different countries around the world, different times, and different topics. The former would be used to train the deep learning model while the latter will be used to perform sentiment analysis on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f234ac",
   "metadata": {},
   "source": [
    "### Collecting the IMDB reviews dataset\n",
    "The IMDB dataset can be found on the website by [Stanford AI Lab](http://ai.stanford.edu/~amaas/data/sentiment/). To make things a bit simpler we're using the dataframe version downloaded from [Kaggle](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)\n",
    "   > The IMDb platform allows the usage of their public datasets for personal and non-commercial use. We did our best to ensure that these reviews do not contain any of the aforementioned sensitive topics pertaining to the reviewer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19307b3c",
   "metadata": {},
   "source": [
    "### Collecting and loading the speech transcripts\n",
    "We have chosen speeches by activists around the globe talking about issues like climate change, feminism, lgbtqa+ rights and racism. These were sourced them from newspapers, the official website of the United Nations and the archives of established universities as cited in the table below. A csv file was created containing the transcribed speeches, their speaker and the source the speeches were obtained from. \n",
    "We made sure to include different demographics in our data and included a range of different topics, most of which focus on social and/or ethical issues. The dataset is subjected to the CCO Creative Common License, which means that is free for the public to use and there are no copyrights reserved.\n",
    "\n",
    "| Speech                                           | Speaker                 | Source                                                     |\n",
    "|--------------------------------------------------|-------------------------|------------------------------------------------------------|\n",
    "| Barnard College Commencement                     | Leymah Gbowee           | Barnard College - Columbia University official website     |\n",
    "| UN Speech on youth Education                     | Malala Yousafzai        | Iowa state university archives                             |\n",
    "| Remarks in the UNGA on racial discrimination     | Linda Thomas Greenfield | United States mission to the United Nation                 |\n",
    "| How Dare You                                     | Greta Thunberg          | NBC’s official website                                     |\n",
    "| The speech that silenced the world for 5 minutes | Severn Suzuki           | NTU blogs                                                  |\n",
    "| The Hope Speech                                  | Harvey Milk             | University of Maryland archives                            |\n",
    "| Violence against LGBTQA+                         | Michelle Bachelet       | United Nations office of high commisioner official website |\n",
    "| I have a dream                                   | Martin Luther King      | Brittanica official website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e8d511",
   "metadata": {},
   "source": [
    "## 2. Preprocess the datasets\n",
    ">Preprocessing data is an extremely crucial step before building any Deep learning model, however in an attempt to keep the tutorial focused on building the model, we will not dive deep into the code for preprocessing. Given below is a brief overview of all the steps we undertake to clean our data and convert it to its numeric representation. The [code](https://github.com/Dbhasin1/ethics-tutorial/blob/lstm-update/tutorials/text_preprocessing.py) is public and we encourage you to look it up for a better understanding of this section\n",
    "\n",
    "1. **Text Denoising** : Before converting your text into vectors, it is important to clean it and remove all unhelpful parts a.k.a the noise from your data by converting all characters to lowercase, removing html tags, brackets and stop words (words that don't add much meaning to a sentence). Without this step the dataset is often a cluster of words that the computer doesn't understand.\n",
    "\n",
    "\n",
    "2. **Tokenization** : So far the text we have is in its raw form, it needs to be broken apart into chunks called tokens because the most common way of processing language happens at the token level. This process of separating a piece of text into smaller units is called Tokenisation. The tokens obtained are then used to build a vocabulary. Vocabulary refers to a set of all tokens in the corpus along with a unique index allotted to each of them.\n",
    "\n",
    "\n",
    "3. **Converting words to vectors** : A word embedding is a learned representation for text where words that have the same meaning have a similar representation. Individual words are represented as real-valued vectors in a predefined vector space. GloVe is n unsupervised algorithm developed by Stanford for generating word embeddings by generating global word-word co-occurence matrix from a corpus. You can download the zipped files containing the embeddings from https://nlp.stanford.edu/projects/glove/. Here you can choose any of the four options for different sizes or training datasets\n",
    " >The GloVe word embeddings include sets that were trained on billions of tokens, some up to 840 billion tokens. These algorithms exhibit stereotypical biases, such as gender bias which can be traced back to the original training data. For example certain occupations seem to be more biased towards a particular gender, reinforcing problematic stereotypes. The nearest solution to this problem are some de-biasing algorithms as the one presented in https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/reports/6835575.pdf which one can use on embeddings of their choice to mitigate bias, if present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd1b7b",
   "metadata": {},
   "source": [
    "You will need two files for the initial preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a0a851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data_path = '../../lstm/server/IMDB Dataset.csv'\n",
    "emb_path = '../../lstm/server/glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c2c597",
   "metadata": {},
   "source": [
    "Next, you will load the IMDB dataset into a dataframe using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b5bf0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df = pd.read_csv(imdb_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d1ef4c",
   "metadata": {},
   "source": [
    "We will use the text preprocessing class imported from the aforementioned [code](https://github.com/Dbhasin1/ethics-tutorial/blob/lstm-update/tutorials/text_preprocessing.py) to carry out the data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dd51e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dbhasin/Desktop/Quansight/ethics-tutorial/tutorials/text_preprocessing.py:84: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data_without_stopwords[f'clean_{text_column}'] = data_without_stopwords[f'clean_{text_column}'].str.replace('[{}]'.format(string.punctuation), ' ')\n"
     ]
    }
   ],
   "source": [
    "imdb_textproc = TextPreprocess(emb_path)\n",
    "X = imdb_textproc.cleantext(imdb_df, 'review', remove_stopwords = True, remove_punc = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b76c11",
   "metadata": {},
   "source": [
    "Now, we need to create a split between training and testing datasets. You can vary the split_percentile to try different ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee676265",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = imdb_df['sentiment'].to_numpy()\n",
    "X_train, Y_train, X_test, Y_test = imdb_textproc.split_data(X, y, split_percentile=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d8a7cf",
   "metadata": {},
   "source": [
    "In order to replace each word with its word embedding, we will first need to replace it with its unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "038b7b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = imdb_textproc.transform_input(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7372d7f8",
   "metadata": {},
   "source": [
    "We will use the training corpus to build a matrice mapping each word index and word embedding. This will act as a cache for when we have to replace each word indice with its respective word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86519893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "imdb_emb_matrix = imdb_textproc.emb_matrix(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77de32a8",
   "metadata": {},
   "source": [
    "Now, we will apply the same process to the speeches in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49502920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "speech_data_path = '../data/speeches.csv'\n",
    "speech_df = pd.read_csv(speech_data_path)\n",
    "speech_textproc = TextPreprocess(emb_path)\n",
    "X_pred = speech_textproc.cleantext(speech_df, 'speech', remove_stopwords = True, remove_punc = False)\n",
    "speakers = speech_df['speaker'].to_numpy()\n",
    "speech_emb_matrix = speech_textproc.emb_matrix(X_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570db92b",
   "metadata": {},
   "source": [
    "### 3. Build the Deep Learning Model¶\n",
    " ---\n",
    " It’s time to start implementing our LSTM! You will have to first familiarize yourself with some high-level concepts of the basic building blocks of a deep learning model. You can refer to the [Deep learning on MNIST from scratch tutorial](https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html) for the same. \n",
    "\n",
    "You will then learn how a Recurrent Neural Network differs from a plain Neural Network and what makes it so suitable for processing sequential data. Afterwards, you will construct the building blocks of a simple deep learning model in Python and NumPy and train it to learn to classify the sentiment of a piece of text as positive or negative with a certain level of accuracy\n",
    "\n",
    "### Introduction to a Long Short Term Memory Network\n",
    "\n",
    "In a plain neural network, the information only moves in one direction — from the input layer, through the hidden layers, to the output layer. The information moves straight through the network and never takes the previous nodes into account at a later stage. Because it only considers the current input, it has no notion of order in time. It simply can’t remember anything about what happened in the past except its training.\n",
    "\n",
    "In a RNN the information cycles through a loop. When it makes a decision, it considers the current input and also what it has learned from the inputs it received previously. \n",
    "\n",
    "The problem with an RNN however, is that the influence of a given input on the hidden layer, and therefore on the network output, either decays or blows up exponentially as it cycles around the network’s recurrent connections. This shortcoming is referred to as the vanishing gradient problem. Long Short-Term Memory (LSTM) is an RNN architecture specifically designed to address the vanishing gradient problem."
   ]
  },
  {
   "attachments": {
    "lstm.jpg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "id": "78698b70",
   "metadata": {},
   "source": [
    "### Overview of the Model Architecture \n",
    "\n",
    "![lstm.jpg](attachment:lstm.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf9c973",
   "metadata": {},
   "source": [
    "In the above image, The rectangles labelled 'A' are called `Cells` and they are the **Memory Blocks** of our LSTM network. They are responsible for choosing what to remember in a sequence and pass on that information to the next cell via two states called the `hidden state` $H_{t}$ and the `cell state` $C_{t}$ where $t$ indicates the time-step. To know how these states are calculated you'll need to understand the mechanisms happening inside a cell, we will recommend you to go through [ Long Short-Term Memory (LSTM)](http://d2l.ai/chapter_recurrent-modern/lstm.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd29ad6a",
   "metadata": {},
   "source": [
    "### But how do we obtain sentiment from the LSTM's output?\n",
    "The hidden state we obtain from the last word in our sequence is considered to be a representation of all the information contained in a sequence. To classify this information into various classes (2 in our case, positive and negative) we can use a Fully Connected layer which firstly maps this information to a desired output size and an activation layer like sigmoid on top of it finally converts the output to a value between 0 and 1. We'll consider values greater than 0.5 to be indicative of a positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f04cc57",
   "metadata": {},
   "source": [
    "Define a function to randomly initialise the parameters which will be learnt while our model trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fc48ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_params (hidden_dim, input_dim):\n",
    "        Wf = np.random.randn(hidden_dim, hidden_dim + input_dim) # forget gate \n",
    "        bf = np.random.randn(hidden_dim, 1)\n",
    "        Wi = np.random.randn(hidden_dim, hidden_dim + input_dim) # input gate \n",
    "        bi = np.random.randn(hidden_dim, 1)\n",
    "        Wcm = np.random.randn(hidden_dim, hidden_dim + input_dim) # candidate memory gate \n",
    "        bcm = np.random.randn(hidden_dim, 1)\n",
    "        Wo = np.random.randn(hidden_dim, hidden_dim + input_dim) # output gate \n",
    "        bo = np.random.randn(hidden_dim, 1)\n",
    "        \n",
    "        W2 = np.random.randn(1, hidden_dim) # fully connected classification layer \n",
    "        b2 = np.zeros((1, 1))\n",
    "\n",
    "        parameters = {\"Wf\": Wf, \"bf\": bf, \"Wi\": Wi, \"bi\": bi, \"Wcm\": Wcm, \"bcm\": bcm, \"Wo\": Wo, \"bo\": bo, \"W2\": W2, \"b2\": b2}\n",
    "        return parameters    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118bbf79",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "\n",
    "Now that we have our initialised parameters we pass the input data in a forward direction through the network. Each layer accepts the input data, processes it and passes it to the successive layer. This process is called `Forward Propagation`. You will undertake the following mechanism to implement the same:\n",
    "- Loading the word embeddings of the input data\n",
    "- Passing the embeddings to an LSTM to obtain the output of the final cell\n",
    "- Passing the final output from the LSTM through a fully connected layer to obtain the probability with which the sequence is positive \n",
    "- Storing all the intermediate outputs in a cache to utilise during backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81fb0e6",
   "metadata": {},
   "source": [
    "Define a function to calculate the sigmoid of an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56d6e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # To prevent overflow \n",
    "    x = np.clip(x, -709.78, 709.78)\n",
    "    res = 1 / (1 + np.exp(-x))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb33914b",
   "metadata": {},
   "source": [
    "Define a function to carry out forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0b47979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop (X_vec, parameters):\n",
    "    \n",
    "    # Loading input data \n",
    "    X_vec = X_vec\n",
    "    \n",
    "    # Retrieve values of all parameters \n",
    "    Wf, bf, Wi, bi, Wcm, bcm, Wo, bo, W2, b2 = parameters.values()\n",
    "    \n",
    "    hidden_dim = Wf.shape[0]\n",
    "    batch_size = X_vec.shape[1]\n",
    "    time_steps = X_vec.shape[2]\n",
    "    \n",
    "    # Initialise hidden and cell state before passing to first time step\n",
    "    prev_hidden_state = np.zeros((hidden_dim, batch_size))\n",
    "    prev_cell_state = np.zeros(prev_hidden_state.shape)\n",
    "    \n",
    "    # Store all the intermediate and final variables here \n",
    "    caches = {'lstm_values':[], 'fc_values':[]}\n",
    "    \n",
    "    # Hidden state from the last cell in the LSTM layer is calculated.\n",
    "    for t in range(time_steps):\n",
    "        # Retrieve embedding for one word for each time step\n",
    "        X_t = X_vec[:, :, t]\n",
    "        \n",
    "        # Concatenate prev_hidden_state and xt\n",
    "        concat = np.vstack((prev_hidden_state, X_t))\n",
    "        \n",
    "        # Calculate output of the forget gate \n",
    "        ft = sigmoid(np.dot(Wf, concat) + bf)\n",
    "        \n",
    "        # Calculate output of the input gate \n",
    "        it = sigmoid(np.dot(Wi, concat) + bi) \n",
    "        cmt = np.tanh(np.dot(Wcm, concat) + bcm)\n",
    "        io = it * cmt \n",
    "        \n",
    "        # Update the cell state \n",
    "        next_cell_state = (ft * prev_cell_state) + io\n",
    "        \n",
    "        # Calculate output of the output gate \n",
    "        ot = sigmoid(np.dot(Wo, concat) + bo)\n",
    "        \n",
    "        # Update the hidden input \n",
    "        next_hidden_state =  ot * np.tanh(next_cell_state)\n",
    "        \n",
    "        # store values needed for backward propagation in cache\n",
    "        cache = (next_hidden_state, next_cell_state, prev_hidden_state, prev_cell_state, ft, it, cmt, ot, X_t)\n",
    "        caches['lstm_values'].append(cache)\n",
    "        \n",
    "        # Update hidden state and cell state for next time step\n",
    "        prev_hidden_state = next_hidden_state\n",
    "        prev_cell_state = next_cell_state\n",
    "\n",
    "    # Pass through a fully connected layer to perform binary classification \n",
    "    z2 = np.dot(W2, next_hidden_state) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    cache = (a2, W2)\n",
    "    caches['fc_values'].append(cache)\n",
    "    \n",
    "    return caches "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aaffc6",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "After each forward pass through the network, you will implement the `backpropagation through time` algorithm to accumulate gradients of each parameter over the time steps. Backpropagation through a LSTM is not as straightforward as through other common Deep Learning architectures, due to the special way its underlying layers interact. Nonetheless, the approach is largely the same; identifying dependencies and applying the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26504e36",
   "metadata": {},
   "source": [
    "Lets start with defining a function to initialise gradients of each parameter as arrays made up of zeros with same dimensions as the corresponding parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef3e2c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the gradients \n",
    "def initialise_grads (parameters):\n",
    "    grads = {}\n",
    "    for param in parameters.keys():\n",
    "        grads[f'd{param}'] = np.zeros((parameters[param].shape))\n",
    "    return grads    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eefb91",
   "metadata": {},
   "source": [
    "Now we'll define a function to calculate the gradients of each intermediate value in the neural network with respect to the loss and accumulate those gradients over the entire sequence. To understand how the gradients are calculated at each step in greater depth, you are suggested to follow this helpful [blog](https://christinakouridi.blog/2019/06/19/backpropagation-lstm/) by Christina Kouridi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e6ede52",
   "metadata": {},
   "outputs": [],
   "source": [
    " def backprop (y, caches, hidden_dim, input_dim, time_steps,  parameters):\n",
    "    # Retrieve output and corresponding weights of fully connected layer\n",
    "    A2, W2 = caches['fc_values'][0]\n",
    "    # Retrieve hidden state calculated in the last time step\n",
    "    h_last = caches['lstm_values'][-1][0]\n",
    "    # Retrieve batch size \n",
    "    batch_size = y.shape[1]\n",
    "    \n",
    "    pred_value = np.array(A2)\n",
    "    target_value = np.array(y)\n",
    "    \n",
    "    # Initialise gradients \n",
    "    dWf, dbf, dWi, dbi, dWcm, dbcm, dWo, dbo, dW2, db2 = initialise_grads(parameters).values()\n",
    "    # Store gradients in a dictionary\n",
    "    grads = {\"dWf\": dWf, \"dbf\": dbf, \"dWi\": dWi, \"dbi\": dbi, \"dWcm\": dWcm, \"dbcm\": dbcm, \n",
    "                 \"dWo\": dWo, \"dbo\": dbo, \"dW2\": dW2, \"db2\": db2}\n",
    "    \n",
    "    # Calculate gradients of the fully connected layer \n",
    "    # dZ2 = dL/da2 * da2/dZ2\n",
    "    dZ2 = pred_value - target_value\n",
    "    # dW2 = dL/da2 * da2/dZ2 * dZ2/dW2\n",
    "    dW2 = (1 / batch_size) * np.dot(dZ2, h_last.T)\n",
    "    # db2 = dL/da2 * da2/dZ2 * dZ2/db2\n",
    "    db2 = (1 / batch_size) * np.sum(dZ2)\n",
    "    \n",
    "    # Gradient of Loss w.r.t the last hidden output of the LSTM \n",
    "    # dh_last = dZ2 * W2 \n",
    "    dh_last = np.dot(W2.T, dZ2)  \n",
    "    \n",
    "    # Initialise gradients w.r.t previous hidden state and cell state \n",
    "    dh_prev = dh_last\n",
    "    dc_prev = np.zeros((dh_prev.shape))\n",
    "    \n",
    "    # loop back over the whole sequence\n",
    "    for t in reversed(range(time_steps)):\n",
    "        cache = caches['lstm_values'][t]\n",
    "        \n",
    "        # Retrieve parameters from \"parameters\"\n",
    "        Wf = parameters[\"Wf\"]\n",
    "        Wi = parameters[\"Wi\"]\n",
    "        Wcm = parameters[\"Wcm\"]\n",
    "        Wo = parameters[\"Wo\"]\n",
    "\n",
    "        # Retrieve information from \"cache\"\n",
    "        (next_hidden_state, next_cell_state, prev_hidden_state, prev_cell_state, ft, it, cmt, ot, X_t) = cache\n",
    "        # Input to gates of LSTM is [prev_hidden_state, X_t]\n",
    "        concat = np.concatenate((prev_hidden_state, X_t), axis=0)\n",
    "        \n",
    "        # Compute gates related derivatives\n",
    "        # Calculate derivative w.r.t the parameters of forget gate \n",
    "        # dft = dL/da2 * da2/dZ2 * dZ2/dh_prev * dh_prev/dc_prev * dc_prev/dft\n",
    "        dft = (dc_prev * prev_cell_state + ot * (1 - np.square(np.tanh(next_cell_state))) * prev_cell_state * dh_prev) * ft * (1 - ft)\n",
    "        # dWf = dft * dft/dWf\n",
    "        dWf = np.dot(dft, concat.T)\n",
    "         # dbf = dft * dft/dbf\n",
    "        dbf = np.sum(dft, axis=1, keepdims=True)\n",
    "        # dh_f = dft * dft/dh_prev\n",
    "        dh_f =  np.dot(Wf[:, :hidden_dim].T, dft)\n",
    "        \n",
    "        # Calculate derivative w.r.t the parameters of input gate \n",
    "        # dit = dL/da2 * da2/dZ2 * dZ2/dh_prev * dh_prev/dc_prev * dc_prev/dit\n",
    "        dit = (dc_prev * cmt + ot * (1 - np.square(np.tanh(next_cell_state))) * cmt * dh_prev) * it * (1 - it)\n",
    "        # dcmt = dL/da2 * da2/dZ2 * dZ2/dh_prev * dh_prev/dc_prev * dc_prev/dcmt\n",
    "        dcmt = (dc_prev * it + ot * (1 - np.square(np.tanh(next_cell_state))) * it * dh_prev) * (1 - np.square(cmt))\n",
    "        # dWi = dit * dit/dWi\n",
    "        dWi = np.dot(dit, concat.T)\n",
    "        # dWcm = dcmt * dcmt/dWcm\n",
    "        dWcm = np.dot(dcmt, concat.T)\n",
    "        # dbi = dit * dit/dbi\n",
    "        dbi = np.sum(dit, axis=1, keepdims=True)\n",
    "        # dWcm = dcmt * dcmt/dbcm\n",
    "        dbcm = np.sum(dcmt, axis=1, keepdims=True)\n",
    "        # dhi = dit * dit/dh_prev\n",
    "        dh_i =  np.dot(Wi[:, :hidden_dim].T, dit)\n",
    "        # dhcm = dcmt * dcmt/dh_prev\n",
    "        dh_cm = np.dot(Wcm[:, :hidden_dim].T, dcmt)\n",
    "        \n",
    "        # Calculate derivative w.r.t the parameters of output gate \n",
    "        # dot = dL/da2 * da2/dZ2 * dZ2/dh_prev * dh_prev/dot \n",
    "        dot = dh_prev * np.tanh(next_cell_state) * ot * (1 - ot)\n",
    "        # dWo = dot * dot/dWo\n",
    "        dWo = np.dot(dot, concat.T)\n",
    "        # dbo = dot * dot/dbo\n",
    "        dbo = np.sum(dot, axis=1, keepdims=True)\n",
    "        # dho = dot * dot/dho\n",
    "        dh_o = np.dot(Wo[:, :hidden_dim].T, dot)\n",
    "       \n",
    "        # Compute derivatives w.r.t previous hidden state and the previous cell state \n",
    "        dh_prev = dh_f + dh_i + dh_cm + dh_o \n",
    "        dc_prev = dc_prev * ft + ot * (1 - np.square(np.tanh(next_cell_state))) * ft * dh_prev\n",
    "        \n",
    "        # sum up the gradients over the sequence \n",
    "        grads[\"dWf\"] += dWf\n",
    "        grads[\"dWi\"] += dWi\n",
    "        grads[\"dWcm\"] += dWcm\n",
    "        grads[\"dWo\"] += dWo\n",
    "        grads[\"dbf\"] += dbf\n",
    "        grads[\"dbi\"] += dbi\n",
    "        grads[\"dbcm\"] += dbcm\n",
    "        grads[\"dbo\"] += dbo\n",
    "    \n",
    "    # Rescale the gradients to improve training stability \n",
    "    for key in grads:\n",
    "        factor= np.linalg.norm(grads[key])\n",
    "        if factor != 0:\n",
    "            grads[key] = grads[key]/factor\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c6f707",
   "metadata": {},
   "source": [
    "### Updating the Parameters \n",
    "\n",
    "We update the parameters through an optimization algorithm called [Adam](https://optimization.cbe.cornell.edu/index.php?title=Adam) which is an extension to stochastic gradient descent that has recently seen broader adoption for deep learning applications in computer vision and natural language processing. Specifically, the algorithm calculates an exponential moving average of the gradient and the squared gradient, and the parameters beta1 and beta2 control the decay rates of these moving averages. Adam has shown increased convergence and robustness over other gradient descent algorithms and is often recommended as the default optimizer for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a7f51",
   "metadata": {},
   "source": [
    "Define a function to initialise the moving averages for each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2835d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the moving averages\n",
    "def initialise_mav (hidden_dim, input_dim):\n",
    "    v = {}\n",
    "    s = {}\n",
    "\n",
    "    grad_keys = [\"dWf\", \"dbf\", \"dWi\", \"dbi\", \"dWcm\", \"dbcm\", \"dWo\", \"dbo\", \"dW2\", \"db2\"]\n",
    "    grad_shapes = [(hidden_dim, hidden_dim + input_dim), (hidden_dim, 1),  # shape of dWf, dbf\n",
    "               (hidden_dim, hidden_dim + input_dim), (hidden_dim, 1),  # shape of dWi, dbi\n",
    "               (hidden_dim, hidden_dim + input_dim), (hidden_dim, 1),  # shape of dWcm, dbcm\n",
    "               (hidden_dim, hidden_dim + input_dim), (hidden_dim, 1),  # shape of dWo, dbo\n",
    "               (1, hidden_dim), (1, 1)]  # shape of dW2, db2\n",
    "\n",
    "    # Initialize dictionaries v, s\n",
    "    for key, shape in zip(grad_keys, grad_shapes):\n",
    "        v[key] = np.zeros(shape)\n",
    "        s[key] = np.zeros(shape)\n",
    "        \n",
    "    # Return initialised moving averages \n",
    "    return v,s \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f80570",
   "metadata": {},
   "source": [
    "Define a function to update the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34ddf43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the parameters using Adam optimization \n",
    "def update_parameters (parameters, gradients, v, s, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    param_keys = list(parameters.keys())\n",
    "    grad_keys = list(gradients.keys())\n",
    "    \n",
    "    for param_key, grad_key in zip(param_keys, grad_keys):\n",
    "        # Moving average of the gradients\n",
    "        v[grad_key] = beta1 * v[grad_key] + (1 - beta1) * gradients[grad_key]\n",
    "\n",
    "        # Moving average of the squared gradients\n",
    "        s[grad_key] = beta2 * s[grad_key] + (1 - beta2) * (gradients[grad_key] ** 2)\n",
    "\n",
    "        # Update parameters\n",
    "        parameters[param_key] = parameters[param_key] - learning_rate * v[grad_key] / np.sqrt(\n",
    "            s[grad_key] + epsilon)\n",
    "    # Return updated parameters and moving averages \n",
    "    return parameters, v, s  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b176bff",
   "metadata": {},
   "source": [
    "### Training the Network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d635172",
   "metadata": {},
   "source": [
    "You will start by initialising all the parameters and hyperparameters being used in your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd81bf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "hidden_dim = 64\n",
    "input_dim = 300\n",
    "time_steps = X_train_indices.shape[1]\n",
    "learning_rate = 0.01\n",
    "epochs = 20\n",
    "parameters = initialise_params(hidden_dim, input_dim)\n",
    "v,s = initialise_mav(hidden_dim, input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d965e003",
   "metadata": {},
   "source": [
    "You will then split your input data into batches as it requires way less memory as compared to loading the complete data set and is way faster than loading the data set one input at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb655159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve total number of input samples (tweets in our case)\n",
    "num_samples = X_train_indices.shape[0]\n",
    "# Obtain the total number of batches \n",
    "no_batches = int(num_samples / batch_size)\n",
    "# Initialise batch vectors of the training data \n",
    "X_batches, y_batches = np.zeros((no_batches, batch_size, time_steps)), np.zeros((no_batches, 1, batch_size))\n",
    "y_train = Y_train.reshape((1, Y_train.shape[0]))\n",
    "for i in range(no_batches):\n",
    "    start_example, end_example = i * batch_size, (i + 1) * batch_size\n",
    "    X_batches[i, :, :] = X_train_indices[start_example:end_example,:] \n",
    "    y_batches[i, :, :] = y_train[:, start_example:end_example] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f67ba1",
   "metadata": {},
   "source": [
    "Define a function that replaces each word indice contained in an array with its corresponding word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68d1432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_vectors(X_b, embedding_matrix):\n",
    "    # Retrieve batch size and number of time steps \n",
    "    batch_size, time_steps = X_b.shape[0], X_b.shape[1]\n",
    "    # Retrieve dimension of word embeddings \n",
    "    emb_dim = embedding_matrix[0].shape[0]\n",
    "    \n",
    "    # Obtain array of dimensions (emb_dim, m, T_x) \n",
    "    # containing word embeddings for corresponding word indice\n",
    "    X_emb = np.zeros((emb_dim, batch_size, time_steps))\n",
    "    for i in range(batch_size):\n",
    "        for t in range(time_steps):\n",
    "            X_emb[:, i, t] = embedding_matrix[int(X_b[i, t])]\n",
    "            \n",
    "    # Return embedding array\n",
    "    return X_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e87124",
   "metadata": {},
   "source": [
    "To optimise your deep learning network, you need to calculate a loss based on how well the model is doing on the training data. Loss value implies how poorly or well a model behaves after each iteration of optimization. <br>\n",
    "Define a function to calculate the loss using [negative log likelihood](http://d2l.ai/chapter_linear-networks/softmax-regression.html?highlight=negative%20log%20likelihood#log-likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f816957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_f(A, Y):\n",
    "    # Retrieve batch size \n",
    "    batch_size = Y.shape[1]\n",
    "    # Implement formula for negative log likelihood \n",
    "    loss = - Y * np.log(A) - (1 - Y) * np.log(1 - A)\n",
    "    # Calculate cost function as the average of losses for one batch\n",
    "    cost = (1 / batch_size) * np.sum(loss)\n",
    "    # Return cost function \n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcdba7f",
   "metadata": {},
   "source": [
    "Set up the neural network's learning experiment with a training loop and start the training process.\n",
    ">Skip running this cell if you already have the trained parameters stored in a `npy` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd661bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished. \t  Loss : 0.7289012569644053\n",
      "Epoch 2 finished. \t  Loss : 0.7207217596417319\n",
      "Epoch 3 finished. \t  Loss : 0.8584854743220655\n",
      "Epoch 4 finished. \t  Loss : 0.7267256014336937\n",
      "Epoch 5 finished. \t  Loss : 0.7241629248444651\n",
      "Epoch 6 finished. \t  Loss : 0.7356888015618882\n",
      "Epoch 7 finished. \t  Loss : 0.6960702086827921\n",
      "Epoch 8 finished. \t  Loss : 0.7737124767084185\n",
      "Epoch 9 finished. \t  Loss : 0.6960691437136022\n",
      "Epoch 10 finished. \t  Loss : 0.7621892056325346\n",
      "Epoch 11 finished. \t  Loss : 0.6933677115026777\n",
      "Epoch 12 finished. \t  Loss : 0.69304922580421\n",
      "Epoch 13 finished. \t  Loss : 0.69304922580421\n",
      "Epoch 14 finished. \t  Loss : 0.7214877539958697\n",
      "Epoch 15 finished. \t  Loss : 0.6939435335684403\n",
      "Epoch 16 finished. \t  Loss : 0.701692748933922\n",
      "Epoch 17 finished. \t  Loss : 0.6933052412454788\n",
      "Epoch 18 finished. \t  Loss : 0.7387709909949653\n",
      "Epoch 19 finished. \t  Loss : 0.6930488384409238\n",
      "Epoch 20 finished. \t  Loss : 0.6933442400050821\n"
     ]
    }
   ],
   "source": [
    "# To store training losses \n",
    "# training_losses = []\n",
    "\n",
    "# This is a training loop.\n",
    "# Run the learning experiment for a defined number of epochs (iterations).\n",
    "for epoch in range(epochs):\n",
    "    #################\n",
    "    # Training step #\n",
    "    #################\n",
    "    training_losses = []\n",
    "    for b in range(no_batches):\n",
    "        # retrieve a single batch and its corresponding target variables \n",
    "        x_b = X_batches[b, :, :]\n",
    "        y_b = y_batches[b, :, :]\n",
    "        # replace word indices with word embeddings before passing to neural network\n",
    "        x_emb = embedding_vectors(x_b, imdb_emb_matrix)\n",
    "        \n",
    "        # Forward propagation/forward pass:\n",
    "        caches = forward_prop(x_emb, parameters)\n",
    "        \n",
    "        # Backward propagation/backward pass:\n",
    "        gradients = backprop(y_b, caches, hidden_dim, input_dim, time_steps,  parameters)\n",
    "        \n",
    "        # Update the weights and biases for the LSTM and fully connected layer \n",
    "        parameters, v, s = update_parameters (parameters, gradients, v, s, learning_rate=learning_rate, \n",
    "                                              beta1=0.999, beta2=0.9, epsilon=1e-8)\n",
    "        \n",
    "        # Measure the training error (loss function) between the actual\n",
    "        # sentiment (the truth) and the prediction by the model.\n",
    "        y_pred = caches['fc_values'][0][0]\n",
    "        loss = loss_f(y_pred, y_b)\n",
    "        # Store training set losses\n",
    "        training_losses.append(loss)\n",
    "        \n",
    "    # Calculate average of training losses for one epoch\n",
    "    mean_cost = np.mean(training_losses)\n",
    "    print(f'Epoch {epoch + 1} finished. \\t  Loss : {mean_cost}')\n",
    "\n",
    "# save the trained parameters to a npy file \n",
    "np.save('parameters.npy',parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5814b01",
   "metadata": {},
   "source": [
    "### Sentiment Analysis on the Speech Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9ba246",
   "metadata": {},
   "source": [
    "Once our model is trained, we can use the updated parameters to start making our predicitons. We break each speech into paragraphs of uniform size before passing them to the Deep Learning model and predicting the sentiment of each paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af827816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store predicted sentiments \n",
    "preds = []\n",
    "para_len = 100\n",
    "\n",
    "# Retrieve trained values of the parameters\n",
    "parameters = np.load('parameters.npy', allow_pickle='TRUE').item()\n",
    "\n",
    "# This is the prediction loop.\n",
    "for text in X_pred:\n",
    "    # retrieve the speaker and corresponding speech\n",
    "    speech = text\n",
    "    # split the speech into a list of words \n",
    "    words = speech.split()\n",
    "    # obtain the total number of paragraphs\n",
    "    no_paras = int(np.ceil(len(words)/para_len))\n",
    "    # split the speech into a list of sentences \n",
    "    sentences = speech_textproc.sent_tokeniser(speech)\n",
    "    # aggregate the sentences into paragraphs\n",
    "    k, m = divmod(len(sentences), no_paras)\n",
    "    agg_sentences = [sentences[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(no_paras)]\n",
    "    paras = [' '.join(sents) for sents in agg_sentences]\n",
    "    # replace each word with its corresponding word indice \n",
    "    X_pred_indices = speech_textproc.transform_input(paras)\n",
    "    # replace word indices with word embeddings before passing to neural network\n",
    "    x_emb = embedding_vectors(X_pred_indices, speech_emb_matrix)\n",
    "    # Forward Propagation\n",
    "    caches = forward_prop(x_emb, parameters)\n",
    "    \n",
    "    # Retrieve the output of the fully connected layer \n",
    "    A2 = caches['fc_values'][0][0]\n",
    "    threshold = np.mean(A2)\n",
    "\n",
    "    # Mark all predictions >0.5 as positive and <0.5 as negative \n",
    "    pred = np.zeros(A2.shape)\n",
    "    indices = np.where(A2 > threshold)  # indices where output > 0.5\n",
    "    pred[indices] = 1  # are set to 1\n",
    "    \n",
    "    # Store predictions \n",
    "    preds.append(pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead3452f",
   "metadata": {},
   "source": [
    "Visualising our predictions using `Matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08224f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis = []\n",
    "data = {'positive':[], 'negative':[]}\n",
    "for i in range(len(preds)):\n",
    "    # Extract the speaker and the corresponding sentiment predictions \n",
    "    speaker = speakers[i]\n",
    "    sentiment = preds[i]\n",
    "    # The speakers will be used to label the x-axis in our plot \n",
    "    x_axis.append(speaker)\n",
    "    # Obtain percentage of paragraphs with positive predicted sentiment \n",
    "    pos_perc = sum(sentiment)/len(sentiment)\n",
    "    # Store positive and negative percentages \n",
    "    data['positive'].append(pos_perc*100)\n",
    "    data['negative'].append(100*(1-pos_perc))    \n",
    "\n",
    "index = pd.Index(x_axis, name='speaker')\n",
    "df = pd.DataFrame(data, index=index)\n",
    "ax = df.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "ax.set_ylabel('percentage')\n",
    "plt.legend(title='labels', bbox_to_anchor=(1.0, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a032cf",
   "metadata": {},
   "source": [
    "### Looking at our Neural Network from an ethical perspective\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6ea4fa",
   "metadata": {},
   "source": [
    "It's crucial to understand that accurately identifying a text's sentiment is not easy primarily because of the complex ways in which humans express sentiment, using irony, sarcasm, humor, or, in social media, abbreviation. Moreover neatly placing text into two categories: 'positive' and 'negative' can be problematic because it is being done without any context. Words or abbreviations can convey very different sentiments depending on age and location, none of which we took into account while building our model.\n",
    "\n",
    "Along with data, there are also growing concerns that data processing algorithms are influencing policy and daily lives in ways that are not transparent and introduce biases. Certain biases such as the [Inductive Bias](https://en.wikipedia.org/wiki/Inductive_bias#:~:text=The%20inductive%20bias%20(also%20known,that%20it%20has%20not%20encountered.&text=The%20kind%20of%20necessary%20assumptions,in%20the%20phrase%20inductive%20bias) are absolutely essential to help a Machine Learning model generalise better, for example the LSTM we built earlier is biased towards preserving contextual information over long sequences which makes it so suitable for processing sequential data. The problem arises when [societal biases](https://hbr.org/2019/10/what-do-we-do-about-the-biases-in-ai) creep into algorithmic predictions. Optimizing Machine algorithms via methods like [hyperparameter tuning](https://en.wikipedia.org/wiki/Hyperparameter_optimization) can then further amplify these biases by learning every bit of information in the data. \n",
    "\n",
    "\n",
    "There are also cases where bias is only in the output and not the inputs (data, algorithm). For example, in sentiment analysis [accuracy tends to be higher on female-authored texts than on male-authored ones]( https://doi.org/10.3390/electronics9020374). End users of sentiment analysis should be aware that its small gender biases can affect the conclusions drawn from it and apply correction factors when necessary. Hence, it is important that demands for algorithmic accountability should include the ability to test the outputs of a system, including the ability to drill down into different user groups by gender, ethnicity and other characteristics, to identify, and hopefully suggest corrections for, system output biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4f09df",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b337a0",
   "metadata": {},
   "source": [
    "You have learned how to build and train a simple Long Short Term Memory network from scratch using just NumPy to perform sentiment analysis.\n",
    "\n",
    "To further enhance and optimize your neural network model, you can consider one of a mixture of the following:\n",
    "\n",
    "- Increase the training sample size by increasing the `split_percentile`.\n",
    "- Alter the architecture by introducing multiple LSTM layers to make the network deeper.\n",
    "- Use a higher epoch size to train longer and add more regularization techniques, such as early stopping, to prevent overfitting.\n",
    "- Introduce a validation set for an unbiased valuation of the model fit.\n",
    "- Apply batch normalization for faster and more stable training.\n",
    "- Tune other parameters, such as the learning rate and hidden layer size.\n",
    "- Replace LSTM with a [Bidirectional LSTM](https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks) to use both left and right context for predicting sentiment.\n",
    "\n",
    "Nowadays, LSTMs have been replaced by the [Transformer](https://jalammar.github.io/illustrated-transformer/) which uses [Attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) to tackle all the problems that plague an LSTM such as as lack of [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning), lack of [parallel training](https://web.stanford.edu/~rezab/classes/cme323/S16/projects_reports/hedge_usmani.pdf) and a long gradient chain for lengthy sequences\n",
    "\n",
    "Building a neural network from scratch with NumPy is a great way to learn more about NumPy and about deep learning. However, for real-world applications you should use specialized frameworks — such as PyTorch, JAX, TensorFlow or MXNet — that provide NumPy-like APIs, have built-in automatic differentiation and GPU support, and are designed for high-performance numerical computing and machine learning."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
